{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACobbJnIR_ni"
   },
   "source": [
    "Notes on usage:\n",
    "\n",
    "- Make sure to [change runtime to GPU](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm). \n",
    "- The transcript will be saved in Files, which you can find in the menu on the left.\n",
    "- Change the number of speakers below if different from two.\n",
    "- Pick a bigger model if you want more accuracy and a smaller model if you want the program to run faster ([more info](https://github.com/openai/whisper#available-models-and-languages)).\n",
    "- If you know the language being spoken is English, then change language to 'English' as this improves performance.\n",
    "\n",
    "\n",
    "High level overview of what's happening here:\n",
    "\n",
    "\n",
    "1.   I'm using Open AI's Whisper model to seperate audio into segments and generate transcripts.\n",
    "2.   I'm then generating speaker embeddings for each segments.\n",
    "3.   Then I'm using agglomerative clustering on the embeddings to identify the speaker for each segment.   \n",
    "\n",
    "Let me know if I can make it better! Author: https://twitter.com/dwarkesh_sp/status/1579672641887408129\n",
    "\n",
    "_Slightly modified by [@zachlatta](https://github.com/zachlatta) to have cleaner Markdown output._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8C9N12TSwfqe",
    "outputId": "eef820f8-f0b4-458d-9e09-6f85f577eb9a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/transcripts_diarization'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EReERut3wkNu"
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/transcripts_diarization/tvd8t-3fiom.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "buGt4moR5Mac"
   },
   "outputs": [],
   "source": [
    "num_speakers = 2 #@param {type:\"integer\"}\n",
    "\n",
    "language = 'English' #@param ['any', 'English']\n",
    "\n",
    "model_size = 'large' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "\n",
    "\n",
    "model_name = model_size\n",
    "if language == 'English' and model_size != 'large':\n",
    "    model_name += '.en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_JzGiMgxcrN",
    "outputId": "cab1f1c2-3be1-4d3c-c6ab-2da65cb0f167"
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/openai/whisper.git\n",
    "# !pip install -q git+https://github.com/pyannote/pyannote-audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0_tup8RAyBy",
    "outputId": "084ed12c-2f85-4846-a2f4-a0b8a5dfd2ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environments/transcript/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
    "# !pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
    "\n",
    "import whisper\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import pyannote.audio\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "# embedding_model = PretrainedSpeakerEmbedding( \n",
    "#     \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "#     device=torch.device(\"cuda\"))\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "up6NvWV-yFot"
   },
   "outputs": [],
   "source": [
    "embedding_model = PretrainedSpeakerEmbedding( \n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DiE3hs3jnTlf"
   },
   "outputs": [],
   "source": [
    "if path[-3:] != 'wav':\n",
    "    subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
    "    path = 'audio.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vdbad9x5CHkC",
    "outputId": "2de4bc5a-9782-4cff-ffd1-6a1af96f2c1e"
   },
   "outputs": [],
   "source": [
    "model = whisper.load_model(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "z4uw8CrovIN1"
   },
   "outputs": [],
   "source": [
    "result = model.transcribe(path)\n",
    "segments = result[\"segments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U1sZYZ_pkV1u"
   },
   "outputs": [],
   "source": [
    "with contextlib.closing(wave.open(path,'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "i9R5bpc3_EOL"
   },
   "outputs": [],
   "source": [
    "audio = Audio()\n",
    "\n",
    "def segment_embedding(segment):\n",
    "    start = segment[\"start\"]\n",
    "    # Whisper overshoots the end timestamp in the last segment\n",
    "    end = min(duration, segment[\"end\"])\n",
    "    clip = Segment(start, end)\n",
    "    waveform, sample_rate = audio.crop(path, clip)\n",
    "    return embedding_model(waveform[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UPnKe_yQPWkd"
   },
   "outputs": [],
   "source": [
    "embeddings = np.zeros(shape=(len(segments), 192))\n",
    "for i, segment in enumerate(segments):\n",
    "    embeddings[i] = segment_embedding(segment)\n",
    "    embeddings = np.nan_to_num(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QHvbUf8sgUVA"
   },
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
    "labels = clustering.labels_\n",
    "for i in range(len(segments)):\n",
    "    segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "k4kitnXJLcsX"
   },
   "outputs": [],
   "source": [
    "def time(secs):\n",
    "    return datetime.timedelta(seconds=round(secs))\n",
    "\n",
    "f = open(\"transcript.txt\", \"w\")\n",
    "for (i, segment) in enumerate(segments):\n",
    "    if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "        f.write(\"\\n\\n**\" + segment[\"speaker\"] + '** ' + str(time(segment[\"start\"])) + \" - \" + str(time(segment[\"end\"])) +':\\n\\n')\n",
    "        f.write(segment[\"text\"][1:] + ' ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i, segment) in enumerate(segments):\n",
    "#     if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "#         print(segment[\"speaker\"],str(time(segment[\"start\"])),str(time(segment[\"end\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "Robrf3zRzkx_",
    "outputId": "a8d6e80f-8ecb-4bce-ddc6-5dd41ba4fb19"
   },
   "outputs": [],
   "source": [
    "# files.download('transcript.md') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "transcribe",
   "language": "python",
   "name": "transcribe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
